{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import keras\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Sample corpus\n",
    "documents = ['Machine learning is the study of computer algorithms that improve automatically through experience.\\\n",
    "Machine learning algorithms build a mathematical model based on sample data, known as training data.\\\n",
    "The discipline of machine learning employs various approaches to teach computers to accomplish tasks \\\n",
    "where no fully satisfactory algorithm is available.',\n",
    "'Machine learning is closely related to computational statistics, which focuses on making predictions using computers.\\\n",
    "The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning.',\n",
    "'Machine learning involves computers discovering how they can perform tasks without being explicitly programmed to do so. \\\n",
    "It involves computers learning from data provided so that they carry out certain tasks.',\n",
    "'Machine learning approaches are traditionally divided into three broad categories, depending on the nature of the \"signal\"\\\n",
    "or \"feedback\" available to the learning system: Supervised, Unsupervised and Reinforcement',\n",
    "'Software engineering is the systematic application of engineering approaches to the development of software.\\\n",
    "Software engineering is a computing discipline.',\n",
    "'A software engineer creates programs based on logic for the computer to execute. A software engineer has to be more concerned\\\n",
    "about the correctness of the program in all the cases. Meanwhile, a data scientist is comfortable with uncertainty and variability.\\\n",
    "Developing a machine learning application is more iterative and explorative process than software engineering.'\n",
    "]\n",
    "\n",
    "documents_df=pd.DataFrame(documents,columns=['documents'])\n",
    "\n",
    "# removing special characters and stop words from the text\n",
    "stop_words_l=stopwords.words('english')\n",
    "documents_df['documents_cleaned']=documents_df.documents.apply(lambda x: \" \".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stop_words_l) )\n",
    "\n",
    "tfidfvectoriser=TfidfVectorizer()\n",
    "tfidfvectoriser.fit(documents_df.documents_cleaned)\n",
    "tfidf_vectors=tfidfvectoriser.transform(documents_df.documents_cleaned)\n",
    "\n",
    "pairwise_similarities=np.dot(tfidf_vectors,tfidf_vectors.T).toarray()\n",
    "pairwise_differences=euclidean_distances(tfidf_vectors)\n",
    "\n",
    "def most_similar(doc_id,similarity_matrix,matrix):\n",
    "    print (f'Document: {documents_df.iloc[doc_id][\"documents\"]}')\n",
    "    print ('\\n')\n",
    "    print ('Similar Documents:')\n",
    "    if matrix=='Cosine Similarity':\n",
    "        similar_ix=np.argsort(similarity_matrix[doc_id])[::-1]\n",
    "    elif matrix=='Euclidean Distance':\n",
    "        similar_ix=np.argsort(similarity_matrix[doc_id])\n",
    "    for ix in similar_ix:\n",
    "        if ix==doc_id:\n",
    "            continue\n",
    "        print('\\n')\n",
    "        print (f'Document: {documents_df.iloc[ix][\"documents\"]}')\n",
    "        print (f'{matrix} : {similarity_matrix[doc_id][ix]}')\n",
    "\n",
    "most_similar(0,pairwise_similarities,'Cosine Similarity')\n",
    "most_similar(0,pairwise_differences,'Euclidean Distance')   \n",
    "\n",
    "print (tfidf_vectors[0].toarray())\n",
    "print (pairwise_similarities.shape)\n",
    "print (pairwise_similarities[0][:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(documents_df.documents_cleaned)\n",
    "tokenized_documents=tokenizer.texts_to_sequences(documents_df.documents_cleaned)\n",
    "tokenized_paded_documents=pad_sequences(tokenized_documents,maxlen=64,padding='post')\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print (tokenized_paded_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "\n",
    "# Nombre del archivo comprimido que quieres cargar\n",
    "nombre_archivo_comprimido = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "# Obtener la ruta del directorio actual de trabajo\n",
    "directorio_actual = os.getcwd()\n",
    "\n",
    "# Ruta completa al archivo comprimido\n",
    "ruta_completa_comprimida = os.path.join(directorio_actual, nombre_archivo_comprimido)\n",
    "\n",
    "# Ruta al archivo descomprimido (sin la extensión .gz)\n",
    "ruta_descomprimida = ruta_completa_comprimida[:-3]\n",
    "\n",
    "# Verificar si el archivo comprimido existe\n",
    "if os.path.exists(ruta_completa_comprimida):\n",
    "    # Descomprimir el archivo .bin.gz si no existe el archivo descomprimido\n",
    "    if not os.path.exists(ruta_descomprimida):\n",
    "        print(\"Descomprimiendo el archivo...\")\n",
    "        try:\n",
    "            with gzip.open(ruta_completa_comprimida, 'rb') as archivo_comprimido:\n",
    "                contenido_comprimido = archivo_comprimido.read()\n",
    "            with open(ruta_descomprimida, 'wb') as archivo_descomprimido:\n",
    "                archivo_descomprimido.write(contenido_comprimido)\n",
    "            print(\"Descompresión completa.\")\n",
    "        except gzip.BadGzipFile:\n",
    "            print(\"El archivo descargado no está en formato gzip. Verifica la integridad del archivo.\")\n",
    "else:\n",
    "    print(\"El archivo comprimido no se encontró en el directorio actual de trabajo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import KeyedVectors\n",
    "t_model = KeyedVectors.load_word2vec_format(ruta_descomprimida, binary=True)\n",
    "print(\"Modelo Word2Vec cargado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating embedding matrix, every row is a vector representation from the vocabulary indexed by the tokenizer index. \n",
    "embedding_matrix=np.zeros((vocab_size,300))\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    if word in t_model:\n",
    "        embedding_matrix[i]=t_model[word]\n",
    "# creating document-word embeddings\n",
    "document_word_embeddings=np.zeros((len(tokenized_paded_documents),64,300))\n",
    "for i in range(len(tokenized_paded_documents)):\n",
    "    for j in range(len(tokenized_paded_documents[0])):\n",
    "        document_word_embeddings[i][j]=embedding_matrix[tokenized_paded_documents[i][j]]\n",
    "document_word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# Crear y ajustar el vectorizador TF-IDF\n",
    "tfidfvectorizer = TfidfVectorizer()\n",
    "tokenized_paded_documents_list = [' '.join(map(str, tokens)) for tokens in tokenized_paded_documents]\n",
    "tfidf_vectors = tfidfvectorizer.fit_transform(tokenized_paded_documents_list)\n",
    "\n",
    "# Calculando embeddings de documentos\n",
    "document_embeddings = np.zeros((len(tokenized_paded_documents), 300))\n",
    "words = tfidfvectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(len(tokenized_paded_documents)):\n",
    "    for j in range(len(words)):\n",
    "        if j in tokenizer.index_word:\n",
    "            word = tokenizer.index_word[j]  # Assuming index_word exists\n",
    "            document_embeddings[i] += embedding_matrix[tokenizer.word_index[word]] * tfidf_vectors[i, j]\n",
    "\n",
    "print(document_embeddings.shape)\n",
    "\n",
    "# Calculando similitudes y distancias entre pares\n",
    "pairwise_similarities = cosine_similarity(document_embeddings)\n",
    "pairwise_differences = euclidean_distances(document_embeddings)\n",
    "\n",
    "# Encontrando documentos más similares\n",
    "most_similar(0, pairwise_similarities, 'Cosine Similarity')\n",
    "most_similar(0, pairwise_differences, 'Euclidean Distance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Cargar los documentos y tokenizarlos\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(documents_df.documents_cleaned)\n",
    "tokenized_documents = tokenizer.texts_to_sequences(documents_df.documents_cleaned)\n",
    "tokenized_paded_documents = pad_sequences(tokenized_documents, maxlen=64, padding='post')\n",
    "\n",
    "# Obtener el tamaño del vocabulario\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Cargar los vectores de palabras pre-entrenados desde el archivo GloVe\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Crear la matriz de incrustaciones\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Calcular embeddings de documentos ponderados por TF-IDF\n",
    "document_embeddings = np.zeros((len(tokenized_paded_documents), 100))\n",
    "words = tfidfvectoriser.get_feature_names_out()\n",
    "\n",
    "for i in range(len(tokenized_paded_documents)):\n",
    "    for j in range(len(words)):\n",
    "        if words[j] in tokenizer.word_index:\n",
    "            word_index = tokenizer.word_index[words[j]]\n",
    "            embedding_vector = embedding_matrix[word_index]\n",
    "            tfidf_value = tfidf_vectors[i, j]\n",
    "            document_embeddings[i] += embedding_vector * tfidf_value\n",
    "\n",
    "print(document_embeddings.shape)\n",
    "\n",
    "# Calcular similitudes y distancias entre pares\n",
    "pairwise_similarities = cosine_similarity(document_embeddings)\n",
    "pairwise_differences = euclidean_distances(document_embeddings)\n",
    "\n",
    "# Encontrar los documentos más similares\n",
    "most_similar(0, pairwise_similarities, 'Cosine Similarity')\n",
    "most_similar(0, pairwise_differences, 'Euclidean Distance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(doc), tags=[i]) for i, doc in enumerate(documents_df.documents_cleaned)]\n",
    "model_d2v = Doc2Vec(vector_size=100,alpha=0.025, min_count=1)\n",
    "  \n",
    "model_d2v.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model_d2v.train(tagged_data,\n",
    "                total_examples=model_d2v.corpus_count,\n",
    "                epochs=model_d2v.epochs)\n",
    "    \n",
    "document_embeddings=np.zeros((documents_df.shape[0],100))\n",
    "\n",
    "for i in range(len(document_embeddings)):\n",
    "    document_embeddings[i]=model_d2v.docvecs[i]\n",
    "    \n",
    "    \n",
    "pairwise_similarities=cosine_similarity(document_embeddings)\n",
    "pairwise_differences=euclidean_distances(document_embeddings)\n",
    "\n",
    "most_similar(0,pairwise_similarities,'Cosine Similarity')\n",
    "most_similar(0,pairwise_differences,'Euclidean Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "document_embeddings = sbert_model.encode(documents_df['documents_cleaned'])\n",
    "\n",
    "pairwise_similarities=cosine_similarity(document_embeddings)\n",
    "pairwise_differences=euclidean_distances(document_embeddings)\n",
    "\n",
    "most_similar(0,pairwise_similarities,'Cosine Similarity')\n",
    "most_similar(0,pairwise_differences,'Euclidean Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import os\n",
    "\n",
    "# Directorio que contiene los archivos de texto\n",
    "directorio = 'ruta/a/tu/directorio'  # Cambiar a la ubicación adecuada\n",
    "\n",
    "# Lista para almacenar los documentos\n",
    "documentos = []\n",
    "\n",
    "# Iterar sobre los archivos en el directorio\n",
    "for archivo in os.listdir(directorio):\n",
    "    if archivo.endswith('.txt'):  # Asegurarse de que solo se toman en cuenta archivos de texto\n",
    "        with open(os.path.join(directorio, archivo), 'r', encoding='utf-8') as f:\n",
    "            documentos.append(f.read())\n",
    "\n",
    "# DataFrame de pandas para almacenar los documentos\n",
    "documentos_df = pd.DataFrame(documentos, columns=['documentos'])\n",
    "\n",
    "# Remover caracteres especiales y palabras vacías del texto\n",
    "stop_words_es = stopwords.words('spanish')\n",
    "documentos_df['documentos_limpios'] = documentos_df.documentos.apply(lambda x: \" \".join(re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚñÑ]', ' ', w).lower() for w in x.split() if re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚñÑ]', ' ', w).lower() not in stop_words_es))\n",
    "\n",
    "# Inicializar y ajustar el vectorizador TF-IDF\n",
    "vectorizador_tfidf = TfidfVectorizer()\n",
    "vectorizador_tfidf.fit(documentos_df.documentos_limpios)\n",
    "vectores_tfidf = vectorizador_tfidf.transform(documentos_df.documentos_limpios)\n",
    "\n",
    "# Calcular la similitud del coseno y la distancia euclidiana entre los vectores TF-IDF\n",
    "similitudes_coseno = np.dot(vectores_tfidf, vectores_tfidf.T).toarray()\n",
    "distancias_euclidianas = euclidean_distances(vectores_tfidf)\n",
    "\n",
    "# Función para encontrar los documentos más similares\n",
    "def mas_similares(doc_id, matriz_similitud, matriz):\n",
    "    print (f'Documento: {documentos_df.iloc[doc_id][\"documentos\"]}')\n",
    "    print ('\\n')\n",
    "    print ('Documentos Similares:')\n",
    "    if matriz == 'Similitud del Coseno':\n",
    "        indices_similares = np.argsort(matriz_similitud[doc_id])[::-1]\n",
    "    elif matriz == 'Distancia Euclidiana':\n",
    "        indices_similares = np.argsort(matriz_similitud[doc_id])\n",
    "    for indice in indices_similares:\n",
    "        if indice == doc_id:\n",
    "            continue\n",
    "        print('\\n')\n",
    "        print (f'Documento: {documentos_df.iloc[indice][\"documentos\"]}')\n",
    "        print (f'{matriz} : {matriz_similitud[doc_id][indice]}')\n",
    "\n",
    "# Mostrar los documentos más similares al primer documento utilizando la similitud del coseno y la distancia euclidiana\n",
    "mas_similares(0, similitudes_coseno, 'Similitud del Coseno')\n",
    "mas_similares(0, distancias_euclidianas, 'Distancia Euclidiana')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
